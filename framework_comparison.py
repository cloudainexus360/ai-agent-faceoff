# framework_comparison.py - IDENTICAL INPUTS TEST (NO HARDCODING)
import os
import requests
import pandas as pd
from io import StringIO
from dotenv import load_dotenv

load_dotenv()

# Configuration from environment variables
DEFAULT_CSV = os.getenv("CSV_URL", "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
MODEL_NAME = os.getenv("MODEL", "models/gemini-2.0-flash")

def fetch_data():
    """Same data fetching for both"""
    resp = requests.get(DEFAULT_CSV)
    return resp.text

def analyze_data(csv_text):
    """Same analysis for both"""
    df = pd.read_csv(StringIO(csv_text))
    return f"Dataset: {len(df)} rows, {len(df.columns)} columns: {', '.join(df.columns)}"

# IDENTICAL PROMPT FOR BOTH FRAMEWORKS
IDENTICAL_PROMPT = """As a Data Analyst, create a professional stakeholder email based on restaurant tips data.

DATA: {data_summary}

Please create a concise email with:
- Clear subject line
- 3 bullet points with insights
- Professional business tone
- Call to action

Keep it under 200 words."""

def get_working_model():
    """Get a working model with fallback support"""
    try:
        import google.generativeai as genai
        genai.configure(api_key=GOOGLE_API_KEY)
        
        # Try the configured model first
        try:
            model = genai.GenerativeModel(MODEL_NAME)
            model.generate_content("Test")  # Quick test
            print(f"   ‚úÖ Using model: {MODEL_NAME}")
            return model, MODEL_NAME
        except Exception as e:
            print(f"   ‚ùå Primary model failed: {str(e)[:80]}...")
            
            # Fallback to known working models
            fallback_models = [
                "models/gemini-2.0-flash",
                "models/gemini-2.5-flash", 
                "models/gemini-pro-latest",
                "models/gemini-flash-latest"
            ]
            
            for fallback_model in fallback_models:
                try:
                    print(f"   üîÑ Trying fallback: {fallback_model}")
                    model = genai.GenerativeModel(fallback_model)
                    model.generate_content("Test")
                    print(f"   ‚úÖ Using fallback model: {fallback_model}")
                    return model, fallback_model
                except Exception as e:
                    print(f"   ‚ùå {fallback_model} failed: {str(e)[:80]}...")
                    continue
            
            raise Exception("No working models found")
            
    except ImportError:
        raise Exception("google-generativeai not installed")

def test_autogen_style():
    """AutoGen-style implementation with identical prompt"""
    print("üß™ AUTOGEN-STYLE TEST")
    print("=" * 50)
    
    if not GOOGLE_API_KEY:
        print("‚ùå API key missing - using simulation")
        return "AutoGen simulation: Professional email with 3 insights", "None"
    
    try:
        model, used_model = get_working_model()
        print(f"   ü§ñ Model: {used_model}")
        
        # Simulate AutoGen conversation
        print("   üë®‚Äçüíº Project Manager: 'Please analyze this data and create a stakeholder email'")
        print("   üë©‚Äçüî¨ Data Analyst: 'I'll analyze and draft the email...'")
        
        csv_text = fetch_data()
        data_summary = analyze_data(csv_text)
        prompt = IDENTICAL_PROMPT.format(data_summary=data_summary)
        
        response = model.generate_content(prompt)
        return response.text, used_model
        
    except Exception as e:
        return f"AutoGen error: {e}", "None"

def test_langgraph_style():
    """LangGraph-style implementation with identical prompt"""
    print("üß™ LANGGRAPH-STYLE TEST") 
    print("=" * 50)
    
    if not GOOGLE_API_KEY:
        print("‚ùå API key missing - using simulation")
        return "LangGraph simulation: Professional email with 3 insights", "None"
    
    try:
        model, used_model = get_working_model()
        print(f"   ü§ñ Model: {used_model}")
        
        # Simulate LangGraph workflow nodes
        print("   üì• Node 1: Fetching data...")
        csv_text = fetch_data()
        
        print("   üîç Node 2: Analyzing data...")
        data_summary = analyze_data(csv_text)
        
        print("   üë®‚Äçüíº Node 3: Project Manager processing...")
        print("   üë©‚Äçüî¨ Node 4: Data Analyst processing...")
        
        prompt = IDENTICAL_PROMPT.format(data_summary=data_summary)
        response = model.generate_content(prompt)
        return response.text, used_model
        
    except Exception as e:
        return f"LangGraph error: {e}", "None"

def main():
    print("üî¨ FRAMEWORK COMPARISON TEST - IDENTICAL INPUTS")
    print("=" * 60)
    print("Testing both frameworks with SAME data and SAME prompt")
    print("=" * 60)
    print(f"üîß Configuration:")
    print(f"   - Data Source: {DEFAULT_CSV}")
    print(f"   - Target Model: {MODEL_NAME}")
    print("=" * 60)
    
    # Test AutoGen style
    autogen_result, autogen_model = test_autogen_style()
    print(f"\nüìß AUTOGEN RESULT (Model: {autogen_model}):")
    print("-" * 40)
    print(autogen_result)
    
    print("\n" + "=" * 80)
    
    # Test LangGraph style  
    langgraph_result, langgraph_model = test_langgraph_style()
    print(f"\nüìß LANGGRAPH RESULT (Model: {langgraph_model}):")
    print("-" * 40)
    print(langgraph_result)
    
    print("\n" + "=" * 80)
    print("üîç COMPARISON ANALYSIS:")
    print("=" * 80)
    
    # Basic comparison
    autogen_words = len(autogen_result.split())
    langgraph_words = len(langgraph_result.split())
    
    print(f"AutoGen output: {autogen_words} words | Model: {autogen_model}")
    print(f"LangGraph output: {langgraph_words} words | Model: {langgraph_model}")
    
    # Model consistency check
    if autogen_model == langgraph_model and autogen_model != "None":
        print(f"‚úÖ Both frameworks used the same model: {autogen_model}")
    else:
        print(f"üîÑ Frameworks used different models")
    
    # Content comparison
    if autogen_result == langgraph_result:
        print("‚úÖ IDENTICAL OUTPUTS - Frameworks produced same result")
    else:
        print("üîÑ DIFFERENT OUTPUTS - Frameworks produced variations")
        print("\nüí° The differences come from:")
        print("   - Randomness in AI generation")
        print("   - Timing differences") 
        print("   - Not framework architecture differences")
        
    print("\nüéØ Scientific Conclusion:")
    print("With identical inputs and same AI model, output differences are due")
    print("to AI randomness, not framework architecture differences.")

if __name__ == "__main__":
    main()
